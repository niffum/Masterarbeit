% Konzept

\chapter{Konzept}
\label{konzept}

Im folgenden werden Konzepte diskutiert, um die in Kapitel \ref{anforderung} herausgearbeiteten Anforderungen zu erfüllen.

\section{Implementierungsziele}
Ziel des Arbeit ist es eine prototypische AR- bzw. VR-Anwendung zur Visualisierung und Untersuchung von MRT-Daten zu implementieren. 

Vor allem im Bereich der Volumendarstellung gibt es bereits viele Programme und Arbeiten, die diese erfolgreich umsetzten, wie im Kapitel \ref{grundlagen} erläutert wurde.
Wie später beschrieben wird, baut mARt auf einer bereits implementierten Lösung auf. Die Anwendung soll mit der Spiele-Engine \textit{Unity}  implementiert werden, worauf ebenfalls im späteren Kapiteln eingegangen wird. Die Implementierung muss also mit der Software kompatibel sein. In Kapitel \ref{grundlagen} wurde unter anderem die \textit{Unity}-Erweiterung \textit{Volume Viewer Pro} beschrieben, die viele der gestellten Anforderungen bereits abdeckt. Das Programm ist allerdings kostenpflichtig. Eine unabhängige und frei zugängliche Software ist dem vorzuziehen. Vor allem, da es sich um einen Prototyp handelt, der Aufschluss über die Verwendungsmöglichkeiten in diesem Bereich liefern und daher eventuell weiterentwickelt werden soll. Deshalb sollte die dreidimensionale Darstellung im Rahmen dieser Arbeit entwickelt werden und Code aus anderen Quellen nur verwendet werden, wenn er diesen Kriterien (z.B. lizenzrechtlichen Bestimmungen) entspricht.

Weiterhin soll die Darstellung in eine interaktive Anwendung eingebettet sein, die die in Kapitel \ref{anforderung} formulierten Anforderungen erfüllt.

\section{3D Darstellung}
\label{3dDarstellung}

Im Kapitel\ref{grundlagen} wurden verschiedene Methoden vorgestellt, um ein Volumen zu visualisieren. An dieser Stelle soll diskutiert werden, welche davon sich am besten für die Umsetzung in mARt eignet.
Die 3D-Darstellung sollte die in Kapitel \ref{anforderung} beschriebenen Eigenschaften haben:

\begin{itemize}
\item Darstellung sollte das Volumen dreidimensional abbilden. \textbf{(U03)}
\item Die innere Struktur des Gehirns sollte erkennbar sein \textbf{(U01)}
\item Die Markierung des vom Schlaganfall betroffenes Bereichs sollte die MRT-Darstellung nicht unkenntlich machen. \textbf{(U12)}
\item Es sollte möglich sein durch die Schichten des Volumes zu scrollen. \textbf{(U07)}
\end{itemize}

Dadurch lassen sich Aussagen über das gewünschte Aussehen der Darstellung herleiten.
Da das Innere der 3D Darstellung erkennbar sein soll, muss eine semi-transparente Darstellung erzeugt werden, die sowohl die Form des Gehirns abbildet, als auch die innere Struktur. Die eingeblendete Markierung sollte ebenfalls zu einen gewissen Grad transparent sein.
Weiterhin sind sind nur die Pixel relevant, die das Gehirn darstellen. Der Bereich darum (Schädel und Hintergrund) sollten gefiltert  und nicht sichtbar gemacht werden. Das gewünschte Ergebnis ist, dass das Gehirn frei im Raum schwebt. 

Die Anforderungen sprechen gegen eine Umsetzung, die eine Oberfläche des Gehirns erzeugt und es als Mesh darstellt. Die Form des Gehirns könnte dadurch deutlich gezeigt werden und das Objekt würde von den umgebenden Pixeln getrennt werden. 
Allerdings handelt es sich bei dem Gehirn nicht um eine Hülle sondern eine Masse. Es ist unwahrscheinlich, dass ein Verfahren wie Marching Cubes die inneren Gehirnstrukturen exakt genug wiedergeben könnte, damit ein Neurologe sinnvolle Schlüsse daraus ziehen kann. Die binäre Aufteilung der Voxel kann außerdem dazu führen, dass Teile des Gehirn fehlerhafter Weise nicht angezeigt werden. Weiterhin müsste das Mesh in Echtzeit kontinuierlich angepasst werden, wenn der Nutzer durch die verschiedenen Schichten scrollt. 
Deshalb eignet sich eine Oberflächengenerierung nicht zur Visualisierung.
Hinzu kommt, dass in den Anforderungen kein Anwendungsfall beschrieben wird, der eine Oberfläche erfordert, wie z.B. die Berechnung einer Kollision des Gehirns mit einem anderen Objekt.	
	
Die dreidimensionale Darstellung lässt sich demnach besser mit einer Volume Rendering Methode umsetzten.  
In Kapitel \ref{grundlagen} wurden die verschiedenen Volume Rendering Techniken miteinander verglichen.
Die Visualisierung der MRT-Daten wurde im Rahmen dieser Arbeit mit den Volume Raycasting Verfahren umgesetzt.
Das Raycasting Verfahren liefert die beste Bildqualität, wie aus dem Vergleich hervorgeht. Es ist daher auch weit verbreitet bei der Implementierung von Volume Rendering Software, wie in Abschnitt \ref{volumeRenderingImplementierung} gezeigt wurde. 
Die verschiedenen Implementierungen können zur Orientierung und als Referenz für die eigene Umsetzung verwendet werden.
Die Technik lässt sich außerdem gut durch einen Fragment-Shader realisieren was eine Implementierung in \textit{Unity} erleichtert. 
Das für Texture-Based Rendering benötigte Texture Slicing und Mapping ist in der Umsetzung nicht trivial und die Ergebnisse des Verfahrens rechtfertigen diesen Aufwand nicht.
Obwohl die Shear-Warp Methode in der Theorie ähnlich abläuft wie das Raycasting, ist die Implementierung komplexer.
Für eine Umsetzung des Volume Renderings im Rahmen dieser Arbeit eignet sich das Raycasting Verfahren deshalb am besten.
Zwar ist die Berechnung einzelner Strahlen leistungsintensiv, kann aber durch Optimierungen und die Verwendung von passender Hardware trotzdem in Echtzeit und interaktiv realisiert werden.

Weiterhin wurden in Abschnitt \ref{klassifikation} die verschiedene Arten erläutert eine Klassifikation zu implementieren. Durch die Umsetzung im Fragment-Shader wird eine Post-Klassifikation umgesetzt. 
Von der Umsetzung einer Pre-Integrierten Klassifikation wurde abgesehen. Zum Einen auf Grund des zusätzliche Implementierungsaufwands, zum Anderen weil die Renderings keine schwerwiegenden Artefakte aufwiesen. Letzteres wird im Abschnitt \ref{ergebnisse} aufgegriffen.

Wie im Kapitel \ref{grundlagen} beschrieben, werden beim Volume Rendering in der Regel Transferfunktionen eingesetzt, um verschiedene Gewebestrukturen oder Materialien zu unterscheiden. Dabei handelt es sich um eine Look-Up-Tabelle in Form einer Textur, die jedem Isowert eine Farbe und Opazität zuweist.
Die Herausforderung bei der Implementierung einer Transferfunktion ist dabei die Generierung einer zum Datensatz passenden Textur. Selbst bei einer Transferfunktion mit nur einer Dimension müssen häufig verschiedene Wertzuweisungen ausprobiert werden, um ein gutes Ergebnis zu erhalten.
Um eine bzw. mehrere Transferfunktionen zu generieren, die den Ansprüchen des Nutzers genügt und die für verschiedene Datensätze passend sind, ist es deshalb sinnvoll dem Nutzer eine Benutzeroberfläche zur Verfügung zu stellen, mit deren Hilfe er die verwendete Transferfunktion zur Laufzeit manipulieren kann. 

In einem Volumen gibt es in der Regel bestimmte Isowerte, die Grenzwerte zwischen verschiedenen Materialien darstellen. Diese dienen als Kontrollpunkte zwischen denen interpoliert wird, um einen Farbverlauf zwischen den Materialien zu erhalten. 
Um die Transfertextur zu beeinflussen muss der Nutzer diese Kontrollpunkte verändern können. Er muss dabei für jeden Kontrollpunkt dessen Iso-, Farb- und Opazitätswert festlegen. Zwischen den Werten der Kontrollpunkte kann dann eine Spline-Interpolation stattfinden.
Die nutzergesteuerte Generierung von Transferfunktionen konnte aus zeitlichen Gründen im Rahmen dieser Arbeit nicht umgesetzt werden.


\section{Darstellung des gekennzeichneten Bereichs}
\label{maske}


In User Story \textbf{U11} der Anforderungen wird beschrieben, dass der vom Schlaganfall betroffene Bereich des Gehirns, der vorher von einem Arzt gekennzeichnet wurde in der Darstellung angezeigt werden soll, wenn dies gewünscht wird. 
Der Bereich wird durch Masken definiert, die zuvor von einem Arzt in einem externen Programm erstellt wurden und ebenfalls im NIfTI-Format vorliegen. Für jede Schicht in einem Datensatz existiert ebenfalls eine Schicht im Maskendatensatz. 

Der Bereich soll sowohl in der zwei- als auch in der dreidimensionalen Ansicht dargestellt werden. Bei letzterer sollte er auch in den Querschnitten erkennbar sein, die sich durch das Scrollen durch das Volumen ergeben. MRT-Daten und markierter Bereich verhalten sich also gleich.

Da die Daten im selben Format vorliegen und sich gleich verhalten sollen, ist es naheliegend, dass sie auf die selbe Weise gerendert werden. Dementsprechend werden auch die Maskenbilder in eine 3D-Textur übertragen, die dann durch Volume Raytracing gerendert wird. Damit aber nicht jeder Strahl doppelt verschossen werden muss, der das Volumen durchdringt, wird der Zugriff auf die Maskentextur in den Shader integriert, der bereits das MRT-Volumen rendert. Für jeden Voxel, der aus der 3D-Textur der MRT-Daten gelesen wird, wird  der Voxel mit denselben Koordinaten aus der Masken-3D-Textur gelesen. Die zwei Farbwerte werden zum Schluss miteinander verrechnet. Durch die Verrechnung, bleibt trotzdem die Struktur des Gehirns erkennbar, wie es von \textbf{U12} gefordert wird. Die Abfrage der Maskenwerte erfolgt nur, wenn durch das Material eine entsprechende Steuervariable gesetzt wurde.
Dies wird im folgenden Codebeispiel veranschaulicht.

\begin{listing}[!htb]
\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{csharp}
void main(sampler3D _Volume,
		  sampler3D _MaskVolume,
          sampler1D _TransferTexture,
          float3 texCoord : TEXCOORD0,
          float4 maskColor : COLOR,
          float _ShowMask)
          {
          
          	// Isowert des Volumens wird für die aktuelle Position ausgelesen
          	float isoValue = tex3D(_Volume, texCoord);
          
          	if (_ShowMask == 1)
          	{
          		// Isowert des Maskenvolumens wird für die 
          		// aktuelle Position ausgelesen
          		float isoValueMask = tex3D(_MaskVolume, texCoord);
          	
          		// Die Maske wird eingefärbt
          		isoValueMask *= maskColor;
          	}
          
          	// Die Werte werden verrechnet
          	return isoValue * isoValueMask;
          }
\end{minted}
\caption{Bestimmung der Farbe eines Pixels in Abhängigkeit zum entsprechenden Isowert sowie dem Iso- und Farbwert der Maskentextur. Letztere werden nur angewandt, wenn die Steuervariable dafür gesetzt wurde. Selbst erstelltes Beispiel}
\label{lst:transfer}
\end{listing}
\FloatBarrier

\section{2D Darstellung}

Neben der dreidimensionalen Ansicht sollen die Daten auch in zwei Dimensionen untersucht werden können, wie es auch in vielen Bildschirmanwendungen der Fall ist.
Die Betrachtung von jeweils nur einer Schicht des Datensatzes erlaubt es den Fokus auf bestimmte Merkmale oder Regionen zu legen, die in dieser Darstellung deutlicher sein könnten als in der volumetrischen. Weiterhin wird der direkte Vergleich zwischen den Szenen deutlicher. Beispielsweise kann die Annahme von der Größe eines Bereichs in der 3D-Darstellung überprüft und so eventuell korrigiert werden.

Da in User Story \textbf{U02} nur das Scrollen auf einer Achse durch die Bildschichten gefordert wird, wird jeder Datensatz als quadratische Bildfläche dargestellt.
Die Anzeige der Daten aus verschiedenen Sichtachsen wäre nicht nur aufwändiger in der Umsetzung, sondern würde die Komplexität der Anwendung auch deutlich steigern, wodurch ihre Benutzbarkeit sinken könnte. 

Die zweidimensionale Darstellung in mARt kann kaum einen Mehrwert gegenüber einer Bildschirmanwendung  bieten. Zwar kann sie zur Untersuchung der Daten genutzt werden, dient allerdings hauptsächlich als komplementäre Ansicht zur 3D-Visualisierung.

\section{Endgerät}
\label{device}

Wie bereits im Abschnitt \ref{motivation} beschrieben, ist mARt als AR-HMD-Anwendung konzipiert. Obwohl die Technologie noch viele Unzulänglichkeiten aufweist, eignet sie sich auf lange Sicht dennoch besser für den Einsatz in einem medizinischen Arbeitsumfeld.
Für den Arbeitsalltag eines Arztes und die Zusammenarbeit mit Patienten eignet sich ein durchsichtiges Display deutlich besser als ein undurchsichtiges. Der Nutzer verliert somit nicht seine Umwelt aus den Augen, wodurch er sich sicherer in der Benutzung fühlt. In einem Anwendungsszenario, in dem er mit anderen Menschen kommuniziert, während er die Anwendung bedient, gilt dies umso mehr. Die mögliche Verwendung von mARt während einer Operation oder Behandlung wäre mit einer VR-Anwendung oder Smartphone-App unmöglich. Die Reduzierung auf die virtuelle Realität bietet zudem keinen Vorteil für die Funktion der Anwendung, da die Umwelt in der Anwendung nicht verändert werden soll. 

Ein weiterer Vorteil eines AR-HMDs ist seine Portabilität. Dies ist vor allem entscheidend, wenn die Daten anderen Personen, wie z.B. Patienten gezeigt werden sollen. Das Gerät lässt sich einfach in ein anderes Zimmer mitnehmen, was mit einem VR-System deutlich schwieriger umzusetzen ist.
 
Eine große Einschränkung bietet die geringe Anzahl an Gesten und Interaktionsmöglichkeiten, die die bisherige AR-Technik zur Verfügung stellt. Dies ist allerdings hinfällig, wenn stattdessen die \textit{Leap Motion} verwendet wird, um Nutzereingaben zu erfassen. Die Vorteile des Gerätes werden im folgenden Abschnitt erläutert.

Es ist zu erwarten, dass sich AR-HMDs in den folgenden Jahren deutlich verbessern werden, was Tragekomfort und Leistungfähigkeit angeht. Da mARt als Prototyp einer medizinischen Anwendung anzusehen ist, kann davon ausgegangen werden, dass bei einer Weiterentwicklung der Anwendung Technologie zur Verfügung steht, die viele der genannten Mängel behebt.

Eine AR-Anwendung eignet sich demnach besser für die Umsetzung. Um diese wiederzugeben wurde die \textit{HoloLens} von \cite{hololens} gewählt, da sie bereits 2015 erschienen und verfügbarer ist.
Die Leistung des Gerätes ist allerdings nicht ausreichend, um die intensiven Berechnungen durchzuführen, die für das Volume Raycasting erforderlich sind. Auch der Einsatz der \textit{Leap Motion} erfordert eine hohe Rechenleistung.
In einer Demo, in der das Volume Rendering auf der \textit{HoloLens} getestet wurde, wurde eine Framerate von 2-10 FPS gemessen, wobei die höhere Rate nur bei einer Darstellung mit kleiner Skalierung erreicht wird. \cite{fpsHololens} empfiehlt eine Framerate von 60 FPS, um die beste Erfahrung zu bieten.
Die Framerate ist nicht ausreichend, um eine angenehme und effektive Arbeitsweise zu ermöglichen. 
Hinzu kommt, dass der Einsatz der \textit{Leap Motion} es verhindert, dass die Anwendung für die \textit{HoloLens} bereitgestellt wird. Dies wird in Abschnitt \ref{kombination} erläutert. Die Anwendung wird deshalb lediglich an die \textit{HoloLens} übertragen, was allerdings eine Latenz mit sich bringt. 
Um im Test eine stabile Darstellung und Interaktion zur Verfügung stellen zu können, wurde die Anwendung zusätzlich als VR-Anwendung umgesetzt. Die Funktionalität bleibt dabei gleich. 
Durch Verwendung der \textit{Leap Motion} ist die Interaktion mit dem Modell in beiden Anwendungen identisch. 
Zur Wiedergabe der VR-Awendung wird die \textit{HTC Vive} verwendet.
% Bezug Hololens2?

\section{Interaktionsdesign} 

In diesem Abschnitt wird das Konzept der Interaktion innerhalb der Anwendung beschrieben. Dazu wird zunächst diskutiert, mit welcher Technik die Nutzereingabe erfasst werden soll. Dann wird auf die Gestaltung der Bedienelemente in der Anwendung eingegangen.

\subsection{Nutzereingabe}
\todo{Referenzen?}

Wie bereits im vorherigen Abschnitt erwähnt, wird die \textit{Leap Motion} in Kombination mit dem jeweiligen HMD benutzt, um den Nutzer mit der Anwendung interagieren zu lassen. 
Die \textit{Leap Motion} verfolgt die Hände des Nutzers und stellt diese als 3D-Modelle innerhalb der Anwendung dar. Die Bewegungen der Hände werden dabei genau nachempfunden, sodass die Illusion entsteht die simulierten Hände wären die eigenen. 
Bereits \cite{Zimmerman86} haben die Hand als natürlichstes Mittel zur Manipulation von Objekten bezeichnet. Zudem haben \cite{Bianchi-Berthouze07} festgestellt, dass Körperbewegung in einer Anwendung zu einem höheren Einsatz in deren Umgang führt. Indem es die Hand in den virtuellen Raum überträgt, fördert die \textit{Leap Motion} den intuitiven Umgang mit der Anwendung enorm und ermöglicht damit ein positives Nutzungserlebnis. Weiterhin wird dadurch das erlernen komplexer Gesten oder die Verwendung von Controllern umgangen. Beides sind Hindernisse, die Nutzer, die mit der Technik nicht vertraut sind abschrecken könnten. 
Nachteilig ist, dass das Sichtfeld der Kamera begrenzt ist. Da diese vorne auf dem HMD angebracht ist, wird dieses noch weiter nach vorne verschoben. Damit seine Hände erkannt werden, muss der Nutzer sie deshalb in einiger Entfernung genau vor sein Gesicht halten. 
Dafür bietet die \textit{Leap Motion} mehr Freiheit in der Interaktion. Dies gilt besonders im Vergleich zu den Eingabemöglichkeiten der \textit{HoloLens}, 
deren Gestenerkennung stark begrenzt ist. Um alle notwendigen Manipulationen der Darstellungen abzudecken, müssten daher entweder deutlich mehr Bedienelemente eingebaut werden oder die Interaktion müsste zu großen Teilen auf Sprachsteuerung beruhen. 
Dies wurde durch Implementierung einer prototypischen \textit{HoloLens}-Anwendung bestätigt, die nur durch die \textit{HoloLens}-Gesten gesteuert werden kann. Sie ist im Kapitel \ref{implementierung} genauer beschrieben.
Ein auf Sprachsteuerung beruhendes Programm ist im Arbeitsalltag eher umständlich, vor allem wenn sich der Anwender im selben Raum wie andere Personen befindet. Zudem wirft es die Problematik der Mehrsprachigkeit auf. 
Allerdings wurde im Frühjahr 2019 ein Nachfolgemodell der \textit{HoloLens} angekündigt, das alle Handbewegungen des Nutzers verfolgt und somit auch Gesten wie Greifen erkennt. Dies wird noch einmal in Abschnitt \ref{hololens2Fazit} erläutert. Es zeigt, dass eine Steuerung durch die Hände des Nutzers nicht nur sinnvoll ist, sondern in der Zukunft auch durch ein HMD umgesetzt werden kann.

Da mARt schließlich in Form einer AR- und VR-Anwendung umgesetzt wird stellt sich weiterhin die Frage nach den Bedienmöglichkeiten in VR. Die Verwendung der \textit{Leap Motion} ermöglicht es hier einerseits, dass beide Anwendung durch dieselben Interaktionen bedient werden können. Andererseits bietet sie auch gegenüber der Steuerung durch Controller Vorteile. 
Wie bereits erwähnt, kann die Bedienung der Controller einen ungeübten Nutzer verwirren. Dazu kommt, dass es sich dabei um eigenständige, für die Anwendung notwendige Geräte handelt. Im Arbeitsalltag müsste immer sicher gestellt werden, dass sie nicht abhanden kommen und zu jedem Zeitpunkt aufgeladen sind. 
Hinzu kommt, dass die Controller zur Verwendung in der Hand gehalten werden müssen. Der Nutzer wird somit der Fähigkeit des Multitasking beraubt, wozu z.B. das Schreiben von Notizen während der Nutzung der Anwendung fällt. Auch die Verwendung von Controllern während einer OP ist ausgeschlossen.

\todo{bezug zu interaktion in grundlagen?}
\subsection{Konzeption der Interaktionselemente}
% 2D 
% UX steht im Vordergrung -> begründen warum besser als vorher!

Im Kapitel \ref{anforderung} ist definiert, welche Interaktionen bzw. Funktionalitäten dem Nutzer zur Verfügung stehen sollten, damit er die Anwendung sinnvoll Nutzen kann. 
Wie vorhergehend beschrieben, wird die \textit{Leap Motion} eingesetzt, um dem Nutzer eine möglichst intuitive Interaktion mit der Anwendung zu ermöglichen. Die Eingabe erfolgt dabei hauptsächlich durch seine Hände, die er frei im virtuellen/augmentierten Raum bewegen und somit mit digitalen Eingabeelementen interagieren kann. 
Dazu stehen ihm einerseits physische Bedienelemente wie Knöpfe, Schieberegler oder Räder zur Verfügung, die virtuell simuliert werden, andererseits die Bewegung der Hände selbst, z.B. durch das Ausüben von Gesten.

Physische Bedienelemente haben den Vorteil, dass sie dem Nutzer bereits aus anderen Kontexten bekannt sind. Durch Icons oder Beschriftungen kann dieser ihre Funktion schnell begreifen. 
\todo{REFERENZ} Dies Ermöglicht eine Verwendung ohne hohen Lernaufwand oder das Merken von Gesten. 
Die Elemente laden außerdem dazu ein sie auszuprobieren, sodass der Nutzer ermutigt wird, mit der Anwendung zu interagieren und sich so mit dieser vertraut zu machen. 
Allerdings ist es schwierig das physische Feedback zu simulieren, das der Nutzer erwartet, da die Elemente der physischen Realität nachempfunden sind. Der Tastsinn des Nutzers kann durch die Anwendung nicht simuliert werden. Es können lediglich audiovisuelle Reize erzeugt werden. Da der Sehsinn allerdings von allen der dominanteste ist \cite{Azmandian16}, kann durch die Verwendung dieser Reize eine immersive Interaktion erreicht werden. 
Schließlich ist zu beachten, dass Interaktionselemente dieser Art Raum innerhalb der virtuellen Realität einnehmen. Dementsprechend ist es wichtig diese so anzuordnen und zu gestalten, dass der virtuelle Raum für den Nutzer übersichtlich bleibt. Hier muss auch ein Gleichgewicht zwischen der Reduzierung der Elemente in Größe und Komplexität, sowie der Bedienbarkeit gefunden werden. Ist ein Knopf beispielsweise zu klein, fällt es schwer diesen zu betätigen. 

Für die Verwendung physischer Bedienelemente spricht außerdem, dass die Komplexität der Interaktionen zu hoch ist, um nur durch Gesten abgedeckt werden zu können. Der intuitive Einsatz der Hände für simple Interaktionen, wie z.B. das Drehen eines Objektes durch Anfassen, ist dabei trotzdem einer abstrakteren Abbildung auf Bedienelemente vorzuziehen. Der Einsatz von einzelnen, eindeutigen Gesten ist weiterhin hilfreich, um der eben genannte visuellen Reizüberflutung entgegen zu wirken. 
Bei der Konzeption der Interaktion mit der Anwendung wird also zuerst angestrebt eine intuitive direkte Bedienmöglichkeit per Hand oder Geste zu verwenden und bei komplexeren Interaktionen möglichst einfach gestaltete physische Bedienelemente einzusetzen.
Im Folgenden wird erläutert, wie die in \ref{anforderung} beschriebenen Interaktionen innerhalb der Anwendung gestaltet wurden.

% Scrollen
Die zentrale Interaktion ist dabei das Scrollen durch die Bildschichten, beschrieben durch \textbf{U07}, die es dem Nutzer ermöglicht einen Einblick in das Innere des Gehirns zu erlangen. In Bildschirmanwendungen wird die Bewegung durch die Schichten meistens durch Betätigung des Scrollrads der Maus erreicht, wie in Kapitel \ref{grundlagen} erläutert wurde.
Dies erfordert geringen Aufwand, ermöglicht es dem Nutzer schnell die Richtung der Bewegung zu ändern und liefert außerdem oft haptisches Feedback. 
Aus diesem Grund ist in Anforderung \textbf{U08} die Verwendung eines Scrollrads zu diesem Zweck beschrieben. Durch die Verwendung der \textit{Leap Motion} entfallen allerdings andere Eingabemittel als die Hände des Nutzers.
Trotzdem sollte der Wechsel zwischen den Schichten in VR/AR möglichst genauso direkt und intuitiv sein. 
Hierzu bieten sich für die 2D- und 3D-Darstellungen verschiedene Möglichkeiten, weswegen die Bedienung sich für diesen Fall in den Szenarien unterscheidet. 

In beiden Darstellungen wird die aktuell angezeigte Schicht durch das Volumen bewegt. Bei einem Volumen lässt sich diese sehr verständlich als Ebene darstellen. Um dieses zu verschieben, ist die direkteste Aktion diese mit der Hand zu greifen und in eine Richtung zu ziehen. 
Dies lässt sich bei der Visualisierung der Daten als Quader genauso umsetzten. Durch die räumliche Perspektive und die Möglichkeit das Volumen zu rotieren lässt sich die Ebene bequem manipulieren. 

Um diese Bedienung darzustellen, wird die Ebene als rechteckiger Rahmen angezeigt. Damit die Daten ungestört untersucht werden können, wird dieser nur dunkel dargestellt, bis sich die Hand des Nutzer dieser nähert. 
Die Position der Berührung von Hand und Rahmen kann schwer zu erfassen sein, vor allem wenn die Ebenen der verschiedenen Achsen dicht beieinander liegen. Dazu kommt außerdem, dass noch andere Funktionen durch des Greifen des Volumens umgesetzt werden. Dazu gehört z.B. die Rotation, wie später noch beschrieben wird. Deshalb ist es sinnvoll dem Nutzer bestimmte Punkte zur Verfügung zu stellen, die er anfassen und somit die Ebenen steuern kann. Diese Punkte werden jeweils an den Eckpunkten der Ebenen angebracht und stehen in einiger Entfernung zum Volumen. Sie reagieren auf die Nähe von und den Kontakt mit Händen. Dies macht es für den Nutzer eindeutig nachvollziehbar, welche Auswirkungen seine Handbewegungen haben werden. 
Die dreidimensionale Darstellung und deren Bedienelemente ist in Abbildung \ref{img:mARt3d} zu dargestellt.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\linewidth]{images/mARt3d_2.png}
	\caption{Die 3D-Darstellung eines MRT-Datensatzes mit angezeigter Maske. Die oberen Schieberegler kontrollieren Intentsität und Gammakorrektur des Renderings. Auch hier wird der markierte Bereich durch ein Icon ein- und ausgeblendet. Jede Ebene kann durch einen Anfassungspunkt verschoben werden.}
	\label{img:mARt3d}
	\source{Eigene Darstellung}
\end{figure}
\FloatBarrier

Dieses Bedienkonzept lässt sich allerdings nicht einfach in den zweidimensionalen Raum übertragen. Da jeweils nur die ausgewählte Schicht zu sehen ist und nicht das gesamte Volumen, hat der Nutzer keine Vorstellung davon wo in dessen Innern er sich befindet. Um ihm diese zu liefern, wird die Gesamtanzahl an schichten angezeigt, sowie die wievielte gerade ausgewählt ist. 
Die Bewegung durch die Schichten erfolgt außerdem entlang der Z-Achse, die durch den zweidimensionalen Kontext allerdings wegfällt. D.h. der Wechsel zwischen den Schichten durch Anfassen und Ziehen eines Kontaktpunktes entlang dieser, wie eben beschrieben, würde nicht in das Konzept passen. 
Weiterhin sollte es dem Nutzer möglich sein die Ebene ununterbrochen zu verschieben. Bei einer Bewegung entlang der Z-Achse müsste er sehr weit vorne anfangen und ist in der Tiefe durch die Länge seines Armes beschränkt. Es wäre möglich das Interaktionselement als Schieberegler in die XY-Achse zu legen, die Bedienung ist dabei allerdings oft ungenau und könnte dazu führen, dass der Nutzer den Fokus auf den Regler legt, anstatt auf das Bild, das er auswählen will. Eine bessere Variante stellt ein Rad dar, durch dessen Rotation man zwischen den Bildern wechseln kann. Dies entspricht eher der Bedienung einer Bildschirmanwendung mittels eines Mausrads und ist dem Nutzer somit bereits bekannt. Außerdem lässt sich so eine kontinuierliche Bewegung in beide Richtungen ermöglichen. Der Nutzer muss zur Steuerung lediglich die Hand und nicht den Arm bewegen, was eine genauere Manipulation erlaubt. 
Das Rad ist dabei ähnlich einer Wählscheibe konzipiert, die der Nutzer mit einer Kreisbewegung rotiert. Verglichen mit einem Drehknopf, wie z.B. einem Lautstärkeregler, unterstützt der größere Radius mehr Kontrolle. Zudem lässt sich eine endlose Bewegung ohne Umgreifen realisieren und die Bewegung ist vom System leichter erkennbar. 
Um dem Nutzer den Eindruck von haptischen Feedback zu vermitteln und den Zustand des Elements zu verdeutlichen, wird die Berührung und Bewegung des Rades durch den Einsatz von Farbe untermalt.

%http://blog.leapmotion.com/designing-cat-explorer/

Wie in Abschnitt \ref{maske} beschrieben, wird der markierte, vom Schlaganfall betroffene Bereich innerhalb des Volumens dargestellt. Da die Maske entweder angezeigt wird oder nicht, kann der Nutzer die gewünschte Darstellung erreichen, indem er einen Schalter bedient, der zwischen den beiden Zuständen wechselt. Dies ist in \textbf{U11} gefordert.

Zur besseren Sichtbarkeit bestimmter Bereiche fordert Anforderung \textbf{U14}, dass die Helligkeit und der Kontrast der MRT-Bilder einstellbar sind. Dies ist über einen Schieberegler steuerbar, der sich oberhalb der Darstellung befindet. 
Die Manipulation von Helligkeit und Kontrast wird dabei als Gammakorrektur im Shader implementiert. Der von Nutzer bestimmte Gammawert wird dazu als Exponent zur Bildfarbe genommen. 
In der 3D-Darstellung wird außerdem ein weiterer Schieberegler angezeigt, über den der Nutzer bestimmen kann, wie intensiv Voxel dargestellt werden sollen. Diese Intensität beeinflusst die Opazität der Voxel. Ist ein hoher Wert eingestellt, werden die äußeren und damit dunkleren Voxel mit geringer Opazität dargestellt, sodass sie die weiter innen liegenden Voxel verdecken. Eine niedrige Intensität hat zur Folge, dass die äußeren Werte ausgeblendet werden, sodass die inneren Strukturen deutlicher zu erkennen sind. Auf diese Weise kann der Nutzer die Umgebung des Gehirns ausblenden und nur das Organ selbst darstellen.
Die Bedienoberfläche zur Manipulation der 2D-MRT-Daten ist in Abbildung \ref{img:mARt2d} zu sehen.

Obwohl nicht in den Anforderungen beschrieben, würde die Manipulation des gekennzeichneten Bereichs mit denselben Optionen dem Nutzer helfen die Darstellung seinen Ansprüchen anzupassen. Dies kann ebenfalls über Schieberegler gesteuert werden. Um die Anwendung überschaubar zu halten wurde allerdings davon abgesehen.


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\linewidth]{images/mARt2d_2.png}
	\caption{Die zweidimensionale Darstellung von zwei MRT-Datensätzen, deren Manipulation nicht gleichgeschaltet ist. Über den oberen Schieberegler können Kontrast und Helligkeit per Gammakorrektur beeinflusst werden. Durch das Icon in der rechten Ecke kann der markierte Bereich ein- und ausgeblendet werden. Das Rad ermöglicht das Scrollen durch die Bildschichten. Die aktuelle Schicht wird in dessen Mitte angezeigt. }
	\label{img:mARt2d}
	\source{Eigene Darstellung}
\end{figure}
\FloatBarrier

% Zoom
Zu den geforderten Interaktionen gehört mit \textbf{U15} auch die Vergrößerung von Bildern, um dem Nutzer eine genaue Untersuchung zu erlauben. Bei der Vergrößerung handelt es sich im Grunde genommen um eine Skalierung des Bildes. Auch dies lässt sich gut mit einer direkten Handbewegung in Berührung mit der Darstellung umsetzten. Aus anderen Anwendungen sind Nutzern zwei Bedienmöglichkeiten dieser Funktionalität bekannt. Bei Bildschirmanwendungen wird oft das Mausrad zum Zoomen verwendet. Allerdings werden Räder bereits für andere Aktionen verwendet und ein mehrdeutiger Einsatz könnte zu Verwirrung führen. Von der Bedienung von Touchdisplays sind Nutzer weiterhin daran gewöhnt zu zoomen, indem sie den Bildschirm mit Daumen und Zeigefinger berühren und die Finger dann aufeinander zu oder voneinander weg bewegen. Diese Bewegung lässt sich gut in die Anwendung übertragen. Um ein eindeutigeres Ergebnis zu erhalten und dem Nutzer mehr Kontrolle zu geben werden statt zwei Fingern allerdings die beiden Hände verwendet. Wird das Bild von beiden Händen mit Daumen und Zeigefinder gegriffen kann es durch die Bewegung der Hände zueinander skaliert werden.

Um zu verhindern, dass zwei Darstellungen sich durch die Skalierung unterscheiden und die Anwendung überschaubar zu halten, ist eine Vergrößerung nur möglich, wenn ein einzelner Datensatz angezeigt wird. 
Das Vergrößern ist dabei als temporäre Manipulation konzipiert. Wenn der Nutzer einen zweiten Datensatz zur Visualisierung auswählt, wird der erste auf seine ursprüngliche Größe zurück gesetzt. Die Skalierung wird auch nicht in die andere Szene übertragen, wenn der Nutzer zwischen 2D und 3D wechselt. Dies kann dem Nutzer auch als bewusstes Zurücksetzten dienen, falls er die Skalierung beispielsweise versehentlich übertrieben hat.

% Verschieben
Die User Story \textbf{U16} fordert die Möglichkeit die Darstellung verschieben zu können. %Ersteres beschreibt dabei die Verschiebung im Raum, zweiteres eine Verschiebung der Darstellung. 
%Die beiden interaktionen wurden zu einer zusammen gefasst. 
Über den Bedienelementen einer Ansicht wird jeweils ein greifbares Objekt platziert. Durch das Greifen, Ziehen und Loslassen des Objektes kann der Nutzer die gesamte Darstellung verschieben. %Damit ist die Positionierung im Raum abgedeckt. 
Vergrößert der Nutzer eine Darstellung kann er dieselbe Verschiebung nutzen, um sich darin zu orientieren. 
%Zwar ist die Verschiebung auf die Armreichweite des Nutzers beschränkt, sofern er sich nicht im Raum bewegt, aber eine übermäßige Skalierung ist generell sowieso nicht erstrebenswert.
Um die Darstellung der Position des Nutzers weiterhin anpassen zu können, kann diese gedreht werden. Dazu dient ein weiteres Bedienelement, welches sich vor der Darstellung befindet. Um die Anwendung übersichtlich zu gestalten, erscheint dieses nur, wenn der Nutzer das erste Element ergriffen hat und somit seine Absicht demonstriert, die Darstellung zu bewegen. 
Sowohl Position als auch Rotation der Ansicht werden beim Szenariowechsel übernommen.
Die Bedienelemente zur Positionierung und Drehung der Ansicht sind in Abbildung \ref{img:3dPos} dargestellt. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\linewidth]{images/mARt_turn.png}
	\caption{Die zweidimensionale Darstellung mit den beiden Bedienelementen zur Positionierung (oben) und Drehung (mitte).}
	\label{img:3dPos}
	\source{Eigene Darstellung}
\end{figure}
\FloatBarrier

Die Darstellung der MRT-Daten soll weiterhin nach \textbf{U17} die ihr gegebene Position behalten, sodass der Nutzer um sie herum gehen und von allen Seiten betrachten kann, oder an einem bestimmten Standpunkt verankern kann. Dies wäre vor allem in einem Multi-Nutzer-Szenario nützlich, in dem mehrere Nutzer gleichzeitig die Daten betrachten.
Die Bedienweise sollte sich durch diese Funktionalität nicht ändern. Zu deren Umsetzung gibt es verschiedene Ansätze, die in Abschnitt \ref{anchor} erläutert werden.

%Drehen
Entsprechend Anforderung \textbf{U13} soll die volumetrische Darstellung der MRT-Daten gedreht werden können. 
Die Drehung erfolgt dabei durch das Greifen des Volumens und Bewegen der Hand. Diese direkte Manipulation ist sehr intuitiv, wenn auch nicht unbedingt genau. Eine gradgenaue Drehung würde bei der Verwendung von mARt allerdings keinen erkennbaren Vorteil bringen. 
Wird das Volumen gedreht, drehen sich die Kontrollpunkte zum Verschieben der Bildebenen mit, um die lokale Verbindung zwischen diesen und der Ebene, die sie manipulieren zu erhalten. Würden sie sich nicht bewegen, wäre es schwer vorauszusehen, welcher Kontrollpunkt welche Ebene beeinflusst. 
Die UI wird von der Rotation allerdings nicht beeinflusst, da ihre Bedienung nur erschwert würde, wenn sie nicht annähernd orthogonal zur Blickrichtung des Nutzer verlaufen würde. 
Ebenso wie die Skalierung, ist die Rotation eine temporäre Manipulation, die beim Szenenwechsel zurückgesetzt wird.

Die User Story \textbf{U04} erfordert es weiterhin, dass Bilder im direkten Vergleich nebeneinander betrachtet werden können. Dies gilt sowohl für die zwei- als auch dreidimensionale Darstellung der Daten. Die User Stories \textbf{U05} und \textbf{U06} erfordern außerdem eine Möglichkeit für den Nutzer aus den vorhandenen Daten einzelne auszuwählen, die angezeigt werden sollen, sowie die Daten jeweils gleichzeitig oder einzeln zu manipulieren. 
Die Auswahl der Datensätze, sowie die Wahl ob diese synchron manipuliert werden sollen oder nicht, sind beide nicht Teil der direkten Manipulation des Bildes. Sie müssen deshalb nicht in dessen unmittelbarem Umfeld stehen und der Nutzer kann während dessen Bedienung auch auf diese den Fokus setzen.
 
Die Bedienung der entsprechenden Interaktionselemente sollte trotzdem intuitiv und schnell umzusetzen sein. Da die Option der Synchronizität abhängig ist von der Tatsache, ob ein oder mehrere Datensätze angezeigt werden, stehen die beiden Aktionen in Verbindung zu einander und werden deshalb in einem Menü vereint, das für beide Darstellungen verwendet wird. Dieses wird an der linken Hand des Nutzers verankert. Auf diese Weise kann es immer schnell erreicht werden und wird nicht unbeabsichtigt aus den Augen verloren. Gleichzeitig fördert es die Immersion der Anwendung, da der Nutzer quasi Teil von ihr wird. Ein Effekt, der in einer Bildschirmanwendung nicht umsetzbar wäre. 

Damit das Menü nicht während der Verarbeitung der Bilder stört, wird es nur dann eingeblendet, wenn der Nutzer seine Handfläche Richtung Kamera dreht und diese somit ansieht. Das \textit{Leap Motion} SDK bietet eine Beispielszene, die dies umsetzt.
Eine besondere Herausforderung bei der Konzeption des Menüs ist es, dieses möglichst einfach und platzsparend zu halten. Der Bereich der \textit{Leap Motion} Kamera, in dem die Hände erkannt werden ist beschränkt. Deshalb kann es bei einer Interaktionsfläche, die viel Raum davon einnimmt dazu kommen, dass der Nutzer versehentlich Knöpfe betätigt. 
Um den Umfang des Menüs in benutzbaren Dimensionen zu halten, wurde es auf drei Knöpfe reduziert. 
Die Liste der verfügbaren Datensätze kann darin allerdings nicht untergebracht werden. Deshalb kann sie bei Bedarf über einen der Knöpfe ausgeklappt werden. 
Indem der Nutzer einen Datensatz auswählt, wird dieser auf der Bildfläche angezeigt. Die Auswahl ist auf zwei Datensätze beschränkt. Zu viele gleichzeitig dargestellte Bilder würden unübersichtlich wirken und die Motivation hinter der User Story ist der direkte Vergleich zweier Bilder. Außerdem stellen mehr Bilder auch mehr Möglichkeiten dar diese in verschiedenen Kombinationen zu manipulieren. Dies hätte die Anwendung unnötig verkompliziert. 
Stattdessen wird die Synchronisierung der Manipulation über einen weiteren Knopf im Handmenü gesteuert. Dieser ist nur aktiv, wenn tatsächlich zwei Bilder angezeigt werden. Dann funktioniert er als Schalter. Indem der Nutzer ihn betätigt wird jeweils nur eine Benutzeroberfläche über beiden Bildern angezeigt oder sie wird dupliziert und für jede Darstellung eingeblendet. 
 
Schließlich kann der Nutzer durch das Handmenü auch zwischen der zwei- und dreidimensionalen Darstellung wechseln, wie es in \textbf{U09} beschrieben ist. Auf dem dafür zuständigen Button wird jeweils das Szenario angezeigt, in das gewechselt wird.
Laut \textbf{U10} sollen beim Wechsel die Manipulationen und Einstellungen möglichst erhalten bleiben. D.h. wenn in 2D zwei Datensätze angezeigt werden, einer mit und einer ohne Maske, sollte das nach dem Wechsel zu 3D ebenfalls so sein. 
Bis auf die genannten Ausnahmen werden die Einstellungen, die in einer Ansicht getätigt werden in die andere übernommen. Handelt es sich um Werte, die nur in einer der beiden Szenarios vorhanden sind, wie die Position der zusätzlichen Ebenen der 3D-Darstellung, werden diese ebenfalls gespeichert.
In Abbildung \ref{img:handUI} ist das ausgeklappte Menü, an der linken Hand dargestellt.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\linewidth]{images/handUI_2.png}
	\caption{Das ausklappbare Handmenü, das erscheint, wenn der Nutzer seine Handfläche zur Kamera dreht. Über die Knöpfe kann der Nutzer die angezeigten Datensätze auswählen (oben und Liste), zwischen 2D- und 3D-Darstellung wechseln (mitte) und die angezeigten Datensätze gleichschalten (unten). }
	\label{img:handUI}
	\source{Eigene Darstellung}
\end{figure}
\FloatBarrier

\subsection{Interaktion AR}

Die Interaktionen bleiben in der AR- und VR-Version der Anwendung gleich, da für beide die \textit{Leap Motion} zur Steuerung verwendet wird.
Im Gegensatz zum Einsatz in VR sind allerdings die realen Hände des Nutzers für diesen sichtbar, während er die Anwendung in AR bedient. Da durch die \textit{Leap Motion} die Hände zusätzlich virtuell simuliert werden, sieht der Nutzer vier Hände, was zuerst verwirrend wirken kann. 
Die Leap-Controller-Hände für den Nutzer unsichtbar zu machen, wäre zwar möglich, ist allerdings nicht die beste Vorgehensweise. Obwohl die \textit{Leap Motion} die Hände und Handbewegungen zuverlässig nachempfindet, verhalten die virtuellen Hände sich nicht ganz deckungsgleich zu den realen. Dies hängt auch damit zusammen, dass die \textit{Leap Motion} Kamera sich ein Stück vor oder über den Augen des Nutzers befindet, da sie am HMD befestigt ist. Dieses Problem wird auch in Abschnitt \ref{kombination} beschrieben.
Die Abweichungen der Simulation sind in einer VR-Anwendung deutlich weniger auffällig. Durch die Immersion der virtuellen Welt und dem Fehlen eines direkten Vergleichs mit der realen, entsteht beim Nutzer die Illusion, dass die virtuellen Hände sich genau dort befinden, wo er seine echten Hände vermutet. Dies ist durch die Dominanz der visuellen Wahrnehmung begründet, wie sie z.B. \cite{Azmandian16} beschreiben.

In AR entfällt diese Illusion. Da die beiden Handpaare aber nicht deckungsgleich sind, kann es dazu kommen, dass ein virtueller Finger einen Knopf trifft, während es ein realer nicht tut oder umgekehrt. Wenn der Nutzer die virtuelle Hand nicht sieht, würde dies den Eindruck von fehlerhaftem Verhalten hervorrufen. 
Aus diesem Grund werden die Leap-Controller-Hände auch in der AR-Anwendung angezeigt. Allerdings werden sie leicht transparent dargestellt, um dem Nutzer zu erlauben auch die dahinter liegende Umgebung wahrzunehmen. 
In Abbildung \ref{img:arHands} sind die Hände innerhalb der Anwendung abgebildet.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\linewidth]{images/AR_hand.jpg}
	\caption{Aufnahme aus der \textit{HoloLens}, die die augmentierte Hand des Nutzers zeigt.}
	\label{img:handUI}
	\source{Eigene Darstellung}
\end{figure}
\FloatBarrier


\section{Unterstützung von Dateiformaten} 

In den im Kapitel \ref{anforderung} beschriebenen Anforderungen ist durch \textbf{U19} und \textbf{U20} beschrieben, dass mARt gängige Dateiformate zur Speicherung von medizinischen Bilddaten, insbesondere NIfTI und DICOM unterstützt. Beide Formate wurden im Kapitel \ref{grundlagen} beschrieben. 
Um die MRT-Bilder verarbeiten und darstellen zu können, muss eine Parser oder Converter in das Programm eingebunden werden. 
Der Nutzer könnte dann über eine Schnittstelle die Daten direkt in die Anwendung laden, wo sie umgewandelt und angezeigt würden. 

In einer realen Arbeitsumgebung ist diese Funktionalität mehr als sinnvoll. Wie allerdings in Kapitel \ref{anforderung} beschrieben, soll es sich bei mARt in erster Linie um einen Prototyp handeln.
Der Fokus soll dabei auf der Darstellung der Daten und der Interaktion mit ihnen liegen. Da der zur Entwicklung und zum Testen zur Verfügung stehende Datensatz gering ist, ist die Nützlichkeit in diesem Szenario nicht gegeben. Der benötigte Zeitaufwand zur Implementierung wird somit nicht durch einen Mehrwert gerechtfertigt.
Weiterhin kann so die Komplexität und der Umfang der Anwendung gering gehalten werden, was es dem Nutzer erlaubt sich auf die Kernfunktionen zu konzentrieren. 

Durch den eingeschränkten verfügbaren Datensatz, konnte auch \textbf{U18}, nicht umgesetzt werden, da jeder Datensatz nur in einer Sequenz zur Verfügung steht.

Trotzdem können mehrere Datensätze in mARt dargestellt werden. Diese werden durch Verwendung von externer Bildsoftware manuell in PNG-Dateien umgewandelt und in die Anwendung integriert, bevor diese gebaut wird. 
Obwohl dieser manuelle Vorgang umständlich ist, ist er für den Zweck der Anwendung und den Umfang dieser Arbeit ausreichend.
